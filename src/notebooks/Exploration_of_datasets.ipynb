{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "35114bf9-156c-4483-b640-249d60ed7709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b0f837-90a8-4445-be41-9bb76452b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOAA dataset\n",
    "# selected 3 regions with 3 stations each - \n",
    "# - specifically with different current levels of bleaching alerts - (HAS TO BE CHANGED)\n",
    "urls = [\n",
    "    # Great Barrier Reef Region\n",
    "    \"https://coralreefwatch.noaa.gov/product/vs/data/gbr_far_northern.txt\",\n",
    "    \"https://coralreefwatch.noaa.gov/product/vs/data/torres_strait.txt\",\n",
    "    \"https://coralreefwatch.noaa.gov/product/vs/data/gbr_northern.txt\",\n",
    "    \n",
    "    # Polynesia Region\n",
    "    \"https://coralreefwatch.noaa.gov/product/vs/data/samoas.txt\",\n",
    "    \"https://coralreefwatch.noaa.gov/product/vs/data/northern_cook_islands.txt\",\n",
    "    \"https://coralreefwatch.noaa.gov/product/vs/data/hawaii.txt\",\n",
    "    \n",
    "    # Caribbean Region\n",
    "    \"https://coralreefwatch.noaa.gov/product/vs/data/nicaragua.txt\",\n",
    "    \"https://coralreefwatch.noaa.gov/product/vs/data/panama_atlantic_east.txt\",\n",
    "    \"https://coralreefwatch.noaa.gov/product/vs/data/jamaica.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5b730-907f-4c46-be4d-c5ac8b1be9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i think this needs to be done in a better way\n",
    "def load_station_data(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # extract station name\n",
    "    lines = response.text.split('\\n')\n",
    "    station_name = \"\"\n",
    "    for i, line in enumerate(lines):\n",
    "        if i == 1 and line.strip():  # station name should be line 1\n",
    "            station_name = line.strip()\n",
    "            break\n",
    "\n",
    "    # region\n",
    "    if 'gbr' in url:\n",
    "        region = 'Great Barrier Reef'\n",
    "    elif any(x in url for x in ['samoas', 'cook', 'hawaiian']):\n",
    "        region = 'Polynesia'\n",
    "    else:\n",
    "        region = 'Caribbean'\n",
    "    \n",
    "    # latitude and longitude\n",
    "    lat = None\n",
    "    lon = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'Latitude' in line and i+1 < len(lines):\n",
    "            try:\n",
    "                lat = float(lines[i+1].strip())\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        if 'Longitude' in line and i+1 < len(lines):\n",
    "            try:\n",
    "                lon = float(lines[i+1].strip())\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    \n",
    "    # Find the data rows more robustly\n",
    "    data_start = 0\n",
    "    headers = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'YYYY' in line and 'MM' in line and 'DD' in line:\n",
    "            headers = line.split()\n",
    "            data_start = i + 1\n",
    "            break\n",
    "    \n",
    "    if not headers or data_start == 0:\n",
    "        raise ValueError(f\"Could not find data headers in {url}\")\n",
    "    \n",
    "    data_rows = []\n",
    "    for line in lines[data_start:]:\n",
    "        if line.strip():  # Skip empty lines\n",
    "            row = line.split()\n",
    "            if len(row) >= 3:  #we need YYYY, MM, DD\n",
    "                if len(row) < len(headers):\n",
    "                    row += [np.nan] * (len(headers) - len(row))\n",
    "                row = row[:len(headers)]\n",
    "                data_rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data_rows, columns=headers)\n",
    "    \n",
    "    # add station information\n",
    "    df['Station'] = station_name if station_name else url.split('/')[-1].replace('.txt', '')\n",
    "    df['Region'] = region\n",
    "    df['Latitude'] = lat\n",
    "    df['Longitude'] = lon\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8d163-119f-4e0e-be47-a740688c43b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from all stations\n",
      "Loaded Far Northern GBR with 14751 records\n",
      "Loaded Torres Strait with 14749 records\n",
      "Loaded Northern GBR with 14751 records\n",
      "Loaded Samoas with 14750 records\n",
      "Loaded Northern Cook Islands with 14751 records\n",
      "Loaded Main Hawaiian Islands with 14751 records\n",
      "Loaded Nicaragua with 14751 records\n",
      "Loaded Panama Atlantic East with 14751 records\n",
      "Loaded Jamaica with 14751 records\n",
      "\n",
      "Final combined dataset shape: (132756, 16)\n",
      "\n",
      "Columns in combined dataset:\n",
      "['YYYY', 'MM', 'DD', 'SST_MIN', 'SST_MAX', 'SST@90th_HS', 'SSTA@90th_HS', '90th_HS>0', 'DHW_from_90th_HS>1', 'BAA_7day_max', 'Station', 'Region', 'Latitude', 'Longitude', 'Date', 'Season']\n",
      "\n",
      "First few rows of combined data:\n",
      "   YYYY  MM  DD  SST_MIN  SST_MAX  SST@90th_HS  SSTA@90th_HS  90th_HS>0  \\\n",
      "0  1985  01  01    28.82    29.25        29.03        0.7797       0.56   \n",
      "1  1985  01  02    28.80    29.33        29.02        0.7135       0.55   \n",
      "2  1985  01  03    28.80    29.27        29.03        0.7219       0.49   \n",
      "3  1985  01  04    28.75    29.31        29.03        0.7103       0.46   \n",
      "4  1985  01  05    28.71    30.67        29.64        1.3177       1.05   \n",
      "\n",
      "   DHW_from_90th_HS>1  BAA_7day_max           Station              Region  \\\n",
      "0                 0.0             0  Far Northern GBR  Great Barrier Reef   \n",
      "1                 0.0             0  Far Northern GBR  Great Barrier Reef   \n",
      "2                 0.0             0  Far Northern GBR  Great Barrier Reef   \n",
      "3                 0.0             0  Far Northern GBR  Great Barrier Reef   \n",
      "4                 0.0             0  Far Northern GBR  Great Barrier Reef   \n",
      "\n",
      "   Latitude  Longitude       Date  Season  \n",
      "0   -12.675      144.1 1985-01-01  Winter  \n",
      "1   -12.675      144.1 1985-01-02  Winter  \n",
      "2   -12.675      144.1 1985-01-03  Winter  \n",
      "3   -12.675      144.1 1985-01-04  Winter  \n",
      "4   -12.675      144.1 1985-01-05  Winter  \n",
      "\n",
      "Combined dataset saved to 'coral_reef_data_combined.csv'\n"
     ]
    }
   ],
   "source": [
    "# load all stations and combine into one dataset\n",
    "print(\"Loading data from all stations\")\n",
    "all_stations_data = []\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        station_df = load_station_data(url)\n",
    "        station_name = station_df['Station'].iloc[0]\n",
    "        print(f\"Loaded {station_name} with {len(station_df)} records\")\n",
    "        all_stations_data.append(station_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {url}: {str(e)}\")\n",
    "\n",
    "\n",
    "if all_stations_data:\n",
    "    combined_df = pd.concat(all_stations_data, ignore_index=True)\n",
    "    \n",
    "    # create date column \n",
    "    if all(col in combined_df.columns for col in ['YYYY', 'MM', 'DD']):\n",
    "        combined_df['YYYY'] = combined_df['YYYY'].astype(str)\n",
    "        combined_df['MM'] = combined_df['MM'].astype(str).str.zfill(2)\n",
    "        combined_df['DD'] = combined_df['DD'].astype(str).str.zfill(2)\n",
    "        \n",
    "        combined_df['Date'] = pd.to_datetime(\n",
    "            combined_df['YYYY'] + '-' + combined_df['MM'] + '-' + combined_df['DD'],\n",
    "            errors='coerce'\n",
    "        )\n",
    "    \n",
    "    numeric_cols = ['SST_MIN', 'SST_MAX', 'SST@90th_HS', 'SSTA@90th_HS', \n",
    "                    '90th_HS>0', 'DHW_from_90th_HS>1', 'BAA_7day_max']\n",
    "    \n",
    "    existing_numeric_cols = [col for col in numeric_cols if col in combined_df.columns]\n",
    "    \n",
    "    for col in existing_numeric_cols:\n",
    "        combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "    \n",
    "    # add season column based on the month\n",
    "    if 'MM' in combined_df.columns:\n",
    "        combined_df['Season'] = combined_df['MM'].apply(lambda x: \n",
    "            'Winter' if x in ['12', '01', '02'] else\n",
    "            'Spring' if x in ['03', '04', '05'] else\n",
    "            'Summer' if x in ['06', '07', '08'] else\n",
    "            'Fall')\n",
    "    \n",
    "    # combined dataset shape\n",
    "    print(f\"\\nFinal combined dataset shape: {combined_df.shape}\")\n",
    "    print(\"\\nColumns in combined dataset:\")\n",
    "    print(combined_df.columns.tolist())\n",
    "    \n",
    "    print(\"\\nFirst few rows of combined data:\")\n",
    "    print(combined_df.head())\n",
    "    \n",
    "    # save\n",
    "    combined_df.to_csv('coral_reef_data_combined.csv', index=False)\n",
    "    print(\"\\nCombined dataset saved to 'coral_reef_data_combined.csv'\")\n",
    "else:\n",
    "    print(\"no stations loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f08fdb82-8805-496c-9603-9c4ff3fefb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCBD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c248cf4f-0c59-44df-a929-bcae97a64bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('global_bleaching_environmental.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5708199a-6497-4690-8e8a-ccea77fcfc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 41,361\n",
      "Total columns: 62\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "54c59e43-6f13-4148-a4b1-83fc8232c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing empty columns\n",
    "df.replace({'nd': np.nan, 'ND': np.nan, '': np.nan}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "871ba041-834c-4aa2-91cd-d99e57228830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 3 cols → 59 left\n"
     ]
    }
   ],
   "source": [
    "null_pct = df.isna().mean() * 100\n",
    "to_drop  = null_pct[null_pct > 90].index.tolist()              # >90% missing\n",
    "const    = df.nunique(dropna=False).loc[lambda x: x <= 1].index # 0 or 1 unique value\n",
    "drop_all = list(set(to_drop) | set(const))\n",
    "df.drop(columns=drop_all, inplace=True)\n",
    "print(f\"Dropped {len(drop_all)} cols → {df.shape[1]} left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d889228c-e063-41dd-8ac7-dc24efe30d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Site_ID  Sample_ID Data_Source  Latitude_Degrees  Longitude_Degrees  \\\n",
      "0     2501   10324336      Donner            23.163           -82.5260   \n",
      "1     3467   10324754      Donner           -17.575          -149.7833   \n",
      "2     1794   10323866      Donner            18.369           -64.5640   \n",
      "3     8647   10328028      Donner            17.760           -64.5680   \n",
      "4     8648   10328029      Donner            17.769           -64.5830   \n",
      "\n",
      "  Ocean_Name Reef_ID            Realm_Name  \\\n",
      "0   Atlantic     NaN     Tropical Atlantic   \n",
      "1    Pacific     NaN  Eastern Indo-Pacific   \n",
      "2   Atlantic     NaN     Tropical Atlantic   \n",
      "3   Atlantic     NaN     Tropical Atlantic   \n",
      "4   Atlantic     NaN     Tropical Atlantic   \n",
      "\n",
      "                               Ecoregion_Name      Country_Name  ... TSA_Mean  \\\n",
      "0                     Cuba and Cayman Islands              Cuba  ...    -2.17   \n",
      "1            Society Islands French Polynesia  French Polynesia  ...    -1.26   \n",
      "2  Hispaniola Puerto Rico and Lesser Antilles    United Kingdom  ...    -1.49   \n",
      "3  Hispaniola Puerto Rico and Lesser Antilles     United States  ...    -1.49   \n",
      "4  Hispaniola Puerto Rico and Lesser Antilles     United States  ...     -1.5   \n",
      "\n",
      "  TSA_Frequency TSA_Frequency_Standard_Deviation TSA_FrequencyMax  \\\n",
      "0             0                             1.09                5   \n",
      "1          0.25                             0.93                4   \n",
      "2             7                             1.31                7   \n",
      "3             3                             0.94                4   \n",
      "4             3                             1.33                5   \n",
      "\n",
      "  TSA_FrequencyMean TSA_DHW  TSA_DHW_Standard_Deviation  TSA_DHWMax  \\\n",
      "0                 0       0                        0.74        7.25   \n",
      "1                 0    0.26                        0.67        4.65   \n",
      "2                 0       0                        1.04       11.66   \n",
      "3                 0       0                        0.75        5.64   \n",
      "4                 0       0                        0.92        6.89   \n",
      "\n",
      "   TSA_DHWMean        Date  \n",
      "0         0.18  2005-09-15  \n",
      "1         0.19  1991-03-15  \n",
      "2         0.26  2006-01-15  \n",
      "3          0.2  2006-04-15  \n",
      "4         0.25  2006-04-15  \n",
      "\n",
      "[5 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7a0e3f3d-8d66-4877-8220-fbb64f44fca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " bleaching related columns\n",
      "Bleaching_Level\n",
      "  Unique values: 1\n",
      "\n",
      "Percent_Bleaching\n",
      "  Unique values: 2271\n",
      "\n",
      "ClimSST\n",
      "  Unique values: 983\n",
      "\n",
      "Temperature_Kelvin\n",
      "  Unique values: 1242\n",
      "\n",
      "Temperature_Mean\n",
      "  Unique values: 813\n",
      "\n",
      "Temperature_Minimum\n",
      "  Unique values: 1023\n",
      "\n",
      "Temperature_Maximum\n",
      "  Unique values: 706\n",
      "\n",
      "Temperature_Kelvin_Standard_Deviation\n",
      "  Unique values: 398\n",
      "\n",
      "SSTA\n",
      "  Unique values: 666\n",
      "\n",
      "SSTA_Standard_Deviation\n",
      "  Unique values: 125\n",
      "\n",
      "SSTA_Mean\n",
      "  Unique values: 1\n",
      "\n",
      "SSTA_Minimum\n",
      "  Unique values: 397\n",
      "\n",
      "SSTA_Maximum\n",
      "  Unique values: 584\n",
      "\n",
      "SSTA_Frequency\n",
      "  Unique values: 414\n",
      "\n",
      "SSTA_Frequency_Standard_Deviation\n",
      "  Unique values: 680\n",
      "\n",
      "SSTA_FrequencyMax\n",
      "  Unique values: 321\n",
      "\n",
      "SSTA_FrequencyMean\n",
      "  Unique values: 153\n",
      "\n",
      "SSTA_DHW\n",
      "  Unique values: 1738\n",
      "\n",
      "SSTA_DHW_Standard_Deviation\n",
      "  Unique values: 591\n",
      "\n",
      "SSTA_DHWMax\n",
      "  Unique values: 2058\n",
      "\n",
      "SSTA_DHWMean\n",
      "  Unique values: 519\n",
      "\n",
      "TSA_DHW\n",
      "  Unique values: 1191\n",
      "\n",
      "TSA_DHW_Standard_Deviation\n",
      "  Unique values: 416\n",
      "\n",
      "TSA_DHWMax\n",
      "  Unique values: 1705\n",
      "\n",
      "TSA_DHWMean\n",
      "  Unique values: 249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# looking for bleaching related columns - we don't need all of these though, so we could just pick the columns we want to keep\n",
    "print(f\"\\n bleaching related columns\")\n",
    "key_terms = ['bleach', 'temperature', 'sst', 'dhw', 'stress', 'severity']\n",
    "for col in df.columns:\n",
    "    if any(term in col.lower() for term in key_terms):\n",
    "        print(f\"{col}\")\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            print(f\"  Range: {df[col].min()} to {df[col].max()}\")\n",
    "        else:\n",
    "            print(f\"  Unique values: {df[col].nunique()}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2520a666-6464-4756-95b8-b22e1baf9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percent_Bleaching summary\n",
      "count    34515.00\n",
      "mean         9.62\n",
      "std         20.19\n",
      "min          0.00\n",
      "25%          0.00\n",
      "50%          0.25\n",
      "75%          6.00\n",
      "max        100.00\n",
      "Name: Percent_Bleaching, dtype: float64\n",
      "  zeros: 16,629, positives: 17,886, missing: 6,846\n"
     ]
    }
   ],
   "source": [
    "df['Percent_Bleaching'] = pd.to_numeric(df['Percent_Bleaching'], errors='coerce')\n",
    "print(\"\\nPercent_Bleaching summary\")\n",
    "print(df['Percent_Bleaching'].describe().round(2))\n",
    "zeros = (df['Percent_Bleaching'] == 0).sum()\n",
    "pos   = (df['Percent_Bleaching'] > 0).sum()\n",
    "miss  = df['Percent_Bleaching'].isna().sum()\n",
    "print(f\"  zeros: {zeros:,}, positives: {pos:,}, missing: {miss:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b196299e-9c8d-457c-afbd-c0e6006d7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorizing the severity of bleaching, might not need this\n",
    "df = df.dropna(subset=['Percent_Bleaching']).copy()\n",
    "df['bleached'] = (df['Percent_Bleaching'] > 0).astype(int)\n",
    "df['severity'] = pd.cut(\n",
    "    df['Percent_Bleaching'],\n",
    "    bins=[-1, 10, 50, 100],\n",
    "    labels=['mild','moderate','severe']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "117ab23f-9ce7-4cef-80e3-94defd8fc088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final dataset:\n",
      "rows: 34,515\n",
      "bleached counts:\n",
      "bleached\n",
      "1    17886\n",
      "0    16629\n",
      "Name: count, dtype: int64\n",
      "severity breakdown:\n",
      "severity\n",
      "mild        27579\n",
      "moderate     4571\n",
      "severe       2365\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal dataset:\")\n",
    "print(f\"rows: {len(df):,}\")\n",
    "print(f\"bleached counts:\\n{df['bleached'].value_counts()}\")\n",
    "print(f\"severity breakdown:\\n{df['severity'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d32e5a62-e535-4ebc-a635-1c3f3872aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_path_csv = 'gcbd_cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9e866a1b-71e3-46c6-9340-1978467975a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned data → gcbd_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(cleaned_path_csv, index=False)\n",
    "print(f\"Saved cleaned data {cleaned_path_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
