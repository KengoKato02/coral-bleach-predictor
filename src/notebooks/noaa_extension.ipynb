{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00e6981",
   "metadata": {},
   "source": [
    "# NOAA: Detecting SST Anomalies Using A Unsupervised Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afeb2ff",
   "metadata": {},
   "source": [
    "***Introduction***\n",
    "\n",
    "The purpose of this notebook serves to further build on our learnings from the noaa_evaluation EDA and attempt to train supervised model training where we will cross validate the results against our random forest regressor using a time-series structure in order to further strengthen the conclusion and results that we have managed to create. We aim to do this by making a comparative analysis between the timestamps of SST anomalies and the prediction results from our Random Forest Regressor that will be tested on the NOAA dataset.\n",
    "\n",
    "Please keep in mind that the noaa_evaluation notebook can be looked at as a mandatory pre-requisite to this part of our findings as we build on the fundamental learnings and results.\n",
    "\n",
    "***Contents***\n",
    "For the contents of this notebook, we can break it down into three main sections Feature Engineering, Unsupervised Training, Cross-Validation of Models\n",
    "\n",
    "- Feature Engineering: This section builds the neccessary feature variables that are used throughout the unsupervised training\n",
    "\n",
    "- Unsupervised Training: This section covers the selection and training of our unsupervised model where we will attempt to accurately detect SST anomalies\n",
    "\n",
    "- Cross-Validation of Models: This section builds on the previous sections model where we shall cross validate it's results with our Random Forest Regressor and test it's predictive accuracy to further prove our models strength\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618aabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "# Custom Tools\n",
    "from load import load_noaa_station_data\n",
    "from utils import create_noaa_date_column, create_noaa_seasonal_column, convert_to_numeric\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Wrangling tools\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c13875",
   "metadata": {},
   "source": [
    "Firstly, let's read the dataset that we pulled and combined for multiple regions and stations in our Multi-Set Analysis (Section 2). This has already be ran through the mini data transformation pipeline that we wrote, so the Season and Date columns are already appended together with the appropriate data type castings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4e33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 177043\n",
      "                YYYY             MM             DD       SST_MIN  \\\n",
      "count  177043.000000  177043.000000  177043.000000  177043.00000   \n",
      "mean     2004.700525       6.487887      15.722932      26.78480   \n",
      "std        11.662090       3.452751       8.797549       1.96167   \n",
      "min      1985.000000       1.000000       1.000000      17.09000   \n",
      "25%      1995.000000       3.000000       8.000000      25.65000   \n",
      "50%      2005.000000       6.000000      16.000000      27.23000   \n",
      "75%      2015.000000       9.000000      23.000000      28.24000   \n",
      "max      2025.000000      12.000000      31.000000      31.44000   \n",
      "\n",
      "             SST_MAX    SST@90th_HS   SSTA@90th_HS      90th_HS>0  \\\n",
      "count  177043.000000  177043.000000  177043.000000  177043.000000   \n",
      "mean       28.117110      27.775844       0.533198       0.187283   \n",
      "std         1.601725       1.627570       0.623586       0.373325   \n",
      "min        21.400000      21.240000      -2.350300       0.000000   \n",
      "25%        27.110000      26.750000       0.126800       0.000000   \n",
      "50%        28.440000      28.090000       0.532300       0.000000   \n",
      "75%        29.320000      29.010000       0.935200       0.190000   \n",
      "max        32.710000      32.590000       3.301000       2.970000   \n",
      "\n",
      "       DHW_from_90th_HS>1   BAA_7day_max       Latitude      Longitude  \n",
      "count       177043.000000  177043.000000  177043.000000  177043.000000  \n",
      "mean             0.922848       0.534791       2.521315      -5.692722  \n",
      "std              2.425705       0.851539      14.953087     122.196208  \n",
      "min              0.000000       0.000000     -19.975000    -170.475000  \n",
      "25%              0.000000       0.000000     -12.675000     -81.750000  \n",
      "50%              0.000000       0.000000       9.200000      69.575000  \n",
      "75%              0.375000       1.000000      17.675000      81.575000  \n",
      "max             25.594300       4.000000      22.225000     145.975000  \n",
      "YYYY                    int64\n",
      "MM                      int64\n",
      "DD                      int64\n",
      "SST_MIN               float64\n",
      "SST_MAX               float64\n",
      "SST@90th_HS           float64\n",
      "SSTA@90th_HS          float64\n",
      "90th_HS>0             float64\n",
      "DHW_from_90th_HS>1    float64\n",
      "BAA_7day_max            int64\n",
      "Station                object\n",
      "Region                 object\n",
      "Latitude              float64\n",
      "Longitude             float64\n",
      "Date                   object\n",
      "Season                 object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "combined_regional_df = pd.read_csv('../../data/noaa/pulled_data/coral_reef_data_combined.csv')\n",
    "\n",
    "print('Number of rows:', len(combined_regional_df))\n",
    "print(combined_regional_df.describe())\n",
    "print(combined_regional_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba175ab",
   "metadata": {},
   "source": [
    "## Section 1: EDA Revision & Extended Analysis\n",
    "\n",
    "### Sub Sections:\n",
    "\n",
    "- Sub-section 1.1: EDA Revision\n",
    "\n",
    "- Sub-section 1.2: Deeper Time-Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cbd98-7217-4138-b09f-9edbdf7c9d49",
   "metadata": {},
   "source": [
    "### Sub-section 1.1: EDA Revision\n",
    "\n",
    "***EDA Recap***\n",
    "\n",
    "Just to give a short recap to what we discovered in our previous findings, if we recall to the Single-Set Anlaysis and particularly our discoveries within the sub-section 1.3 of our distdistributionribtuion analysis, you will see that we found a normal distribution when analysing the SSTA@90th_HS>0. \n",
    "\n",
    "We logically built on that finding by mapping the seasonal fluctuations via grouping the columns by seasons then re-plotting the distributions from each season in the subsequent sub-section 1.4. It was from this analysis that we were able to conclude that the SST anomaly distribution was in fact ***not*** influenced by seasonal changes. \n",
    "\n",
    "Another key insight and derived data that we were able to retrieve from this section can be seen in sub-section 1.5 where we made a time-series analysis and pinpointed the exact time delays measured in days for the maximum correlation that these results had with when mapped to the BAA severity levels. We will also be leveraging these results and building the majority of the next sub-section from this finding.\n",
    "\n",
    "***Building on Those Concepts***\n",
    "\n",
    "It is within this section that we can truly leverage the findings from sub-section 1.4 and 1.5 by building on the concept and trying to engineer ourselves a model that will help detect SST anomalies before they even happen. \n",
    "\n",
    "The target of this build is to not only further strengthen the conclusions drawn for our prior analysis, but to also build an extended model that will allow individuals and scientists to predict SST anomalies before they occur, ultimately creating an early warning system for potential SST anomalies with hopes to be married \n",
    "together with our RFR prediction model from previous notebook.\n",
    "\n",
    "Before we dive into the feature engineering of our unsupervised learning model, we need to particularly revisit a single part of our EDA analysis from the first notebook. This is will be the time-series analysis from subsections 1.5 and 2.5. We must do this as it is the pinnacle for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c28ee6",
   "metadata": {},
   "source": [
    "### Sub-section 1.2: Deep Time-Series Analysis\n",
    "\n",
    "Within this revisit, we will be taking a deeper dive into the time-series lags for the correlations that the values have with not only the BAA severity levels, but with each other. \n",
    "\n",
    "As we will be focusing particularly onto the SST anomalies within this section, we would like to do a time-series analysis around the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e47c90",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad825b",
   "metadata": {},
   "source": [
    "Coming int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66249d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f839e45",
   "metadata": {},
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12155f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
